#!/usr/bin/env python3
"""
Meridian ML Service ç»Ÿä¸€æµ‹è¯•å¥—ä»¶
"""

import json
import time
import requests
import numpy as np
from typing import List, Dict, Any, Optional

# é…ç½®
DEFAULT_BASE_URL = "http://localhost:8081"
DEFAULT_API_TOKEN = "dev-token-123"

# æµ‹è¯•æ•°æ®
TEST_TEXTS = [
    # ç§‘æŠ€æ–°é—» - AI/ML
    "OpenAIå‘å¸ƒäº†æ–°çš„GPT-4æ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›",
    "Googleæ¨å‡ºæ–°çš„AIæœç´¢ç®—æ³•ï¼Œå¤§å¹…æå‡æœç´¢å‡†ç¡®æ€§",
    "Metaå‘å¸ƒVRå¤´æ˜¾æ–°å“ï¼Œæ”¯æŒå…ˆè¿›çš„AIæ··åˆç°å®åŠŸèƒ½",
    "ç™¾åº¦å‘å¸ƒæ–‡å¿ƒä¸€è¨€å¤§æ¨¡å‹ï¼ŒæŒ‘æˆ˜ChatGPTåœ¨ä¸­æ–‡å¸‚åœºçš„åœ°ä½",
    "è‹±ä¼Ÿè¾¾å‘å¸ƒæ–°ä¸€ä»£AIèŠ¯ç‰‡ï¼Œæ€§èƒ½æå‡300%",
    
    # ç§‘æŠ€æ–°é—» - æ¶ˆè´¹ç”µå­
    "è‹¹æœå…¬å¸å‘å¸ƒæœ€æ–°iPhone 15ï¼Œæ­è½½å…ˆè¿›çš„A17èŠ¯ç‰‡",
    "ç‰¹æ–¯æ‹‰å‘å¸ƒè‡ªåŠ¨é©¾é©¶è½¯ä»¶æ›´æ–°ï¼Œæå‡å®‰å…¨æ€§èƒ½",
    "åä¸ºå‘å¸ƒMate 60ç³»åˆ—ï¼Œæ­è½½è‡ªç ”5GèŠ¯ç‰‡",
    "å°ç±³å‘å¸ƒæ–°æ¬¾æ™ºèƒ½æ‰‹æœºï¼Œå”®ä»·ä»…999å…ƒ",
    "ä¸‰æ˜Ÿå±•ç¤ºæŠ˜å å±æŠ€æœ¯æ–°çªç ´ï¼Œå±å¹•å¯æŠ˜å ä¸‡æ¬¡",
    
    # ç»æµæ–°é—»
    "ç¾è”å‚¨å®£å¸ƒç»´æŒåˆ©ç‡ä¸å˜ï¼Œå¸‚åœºååº”å¹³ç¨³",
    "æ¬§æ´²å¤®è¡Œè€ƒè™‘è°ƒæ•´è´§å¸æ”¿ç­–åº”å¯¹æŒç»­é€šèƒ€",
    "ä¸­å›½å¤®è¡Œä¸‹è°ƒå­˜æ¬¾å‡†å¤‡é‡‘ç‡ï¼Œé‡Šæ”¾æµåŠ¨æ€§",
    "çŸ³æ²¹ä»·æ ¼å› åœ°ç¼˜æ”¿æ²»ç´§å¼ å±€åŠ¿ä¸Šæ¶¨5%",
    "æ¯”ç‰¹å¸ä»·æ ¼çªç ´65000ç¾å…ƒï¼Œåˆ›å†å²æ–°é«˜",
]


class MLServiceTester:
    """MLæœåŠ¡ç»Ÿä¸€æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = DEFAULT_BASE_URL, api_token: str = DEFAULT_API_TOKEN):
        self.base_url = base_url
        self.headers = {"X-API-Token": api_token, "Content-Type": "application/json"}
        
    def test_health(self) -> bool:
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        print("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥...")
        
        try:
            response = requests.get(f"{self.base_url}/health", timeout=10)
            
            if response.status_code == 200:
                health_data = response.json()
                print("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
                print(f"   çŠ¶æ€: {health_data['status']}")
                print(f"   åµŒå…¥æ¨¡å‹: {health_data['embedding_model']}")
                print(f"   èšç±»å¯ç”¨: {health_data['clustering_available']}")
                print(f"   ä¼˜åŒ–å¯ç”¨: {health_data['optimization_available']}")
                
                if not health_data['clustering_available']:
                    print("âš ï¸  è­¦å‘Š: èšç±»åŠŸèƒ½ä¸å¯ç”¨")
                    
                return health_data['clustering_available']
            else:
                print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def test_embeddings(self, texts: List[str]) -> Optional[List[List[float]]]:
        """æµ‹è¯•åµŒå…¥ç”Ÿæˆ"""
        print(f"\nğŸ§® æµ‹è¯•åµŒå…¥ç”Ÿæˆ ({len(texts)} ä¸ªæ–‡æœ¬)...")
        
        payload = {"texts": texts}
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/embeddings",
                json=payload,
                headers=self.headers,
                timeout=60
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                embeddings = data['embeddings']
                print(f"âœ… åµŒå…¥ç”ŸæˆæˆåŠŸ")
                print(f"   æ¨¡å‹: {data['model_name']}")
                print(f"   åµŒå…¥ç»´åº¦: {len(embeddings[0])}ç»´")
                print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                return embeddings
            else:
                print(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ åµŒå…¥ç”Ÿæˆå¼‚å¸¸: {e}")
            return None

    def test_clustering(self, texts: List[str]) -> Optional[Dict[str, Any]]:
        """æµ‹è¯•AI Workerèšç±»åŠŸèƒ½"""
        print(f"\nğŸ”— æµ‹è¯•AI Workerèšç±»åŠŸèƒ½ ({len(texts)} ä¸ªæ–‡æœ¬)...")
        
        # é¦–å…ˆç”ŸæˆåµŒå…¥å‘é‡
        embeddings_data = self.test_embeddings(texts)
        if not embeddings_data:
            print("âš ï¸  æ— æ³•è·å–åµŒå…¥å‘é‡ï¼Œè·³è¿‡èšç±»æµ‹è¯•ã€‚")
            return None

        # æ„å»ºAI Workeræ ¼å¼çš„æ•°æ®é¡¹
        items = []
        for i, (text, emb) in enumerate(zip(texts, embeddings_data)):
            items.append({
                "id": i,
                "embedding": emb,
                "title": text[:50] + "..." if len(text) > 50 else text,
                "url": f"http://example.com/article/{i}",
                "content": text
            })
        
        # æ„å»ºæ­£ç¡®çš„è¯·æ±‚payload
        payload = {
            "items": items,
            "config": {
                "umap_n_components": min(8, len(items) - 1),  # è°ƒæ•´ç»´åº¦
                "umap_n_neighbors": min(10, len(items) - 1),  # è°ƒæ•´é‚»å±…æ•°
                "umap_min_dist": 0.0,
                "hdbscan_min_cluster_size": max(2, len(items) // 5),  # æ›´å°çš„æœ€å°ç°‡å¤§å°
                "hdbscan_min_samples": 2,  # å‡å°‘æœ€å°æ ·æœ¬æ•°
                "normalize_embeddings": True
            },
            "optimization": None,
            "content_analysis": {
                "enabled": True,
                "top_n_per_cluster": 3
            }
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/ai-worker/clustering",
                json=payload,  # å‘é€å®Œæ•´çš„payload
                headers=self.headers,
                timeout=120,
                params={
                    "return_embeddings": False,
                    "return_reduced_embeddings": True
                }
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                stats = data['clustering_stats']
                print(f"âœ… AI Workerèšç±»æˆåŠŸ")
                print(f"   ç°‡æ•°é‡: {stats['n_clusters']}")
                print(f"   å¼‚å¸¸ç‚¹: {stats['n_outliers']} ({stats['outlier_ratio']:.1%})")
                print(f"   ç°‡å¤§å°: {stats['cluster_sizes']}")
                if stats.get('dbcv_score') is not None:
                    print(f"   DBCVåˆ†æ•°: {stats['dbcv_score']:.4f}")
                print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                
                # æ˜¾ç¤ºç°‡å†…å®¹ç¤ºä¾‹
                if data.get('clusters'):
                    print(f"   ç°‡å†…å®¹ç¤ºä¾‹:")
                    for cluster in data['clusters'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç°‡
                        cluster_id = cluster['cluster_id']
                        items_in_cluster = cluster['items']
                        print(f"     ç°‡{cluster_id}: {len(items_in_cluster)}ä¸ªæ–‡æœ¬")
                        for item in items_in_cluster[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                            # ä¼˜å…ˆæ˜¾ç¤ºtextå­—æ®µï¼Œç„¶åæ˜¯metadataä¸­çš„title
                            text_content = (item.get('text') or 
                                          item.get('metadata', {}).get('title') or 
                                          item.get('title') or 
                                          'N/A')
                            print(f"       - {text_content[:80]}...")
                
                return data
            else:
                print(f"âŒ AI Workerèšç±»å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ AI Workerèšç±»å¼‚å¸¸: {e}")
            return None

    def test_optimized_clustering(self, texts: List[str]) -> Optional[Dict[str, Any]]:
        """æµ‹è¯•å‚æ•°ä¼˜åŒ–èšç±»åŠŸèƒ½"""
        print(f"\nğŸ¯ æµ‹è¯•å‚æ•°ä¼˜åŒ–èšç±»åŠŸèƒ½ ({len(texts)} ä¸ªæ–‡æœ¬)...")
        
        # é¦–å…ˆç”ŸæˆåµŒå…¥å‘é‡
        embeddings_data = self.test_embeddings(texts)
        if not embeddings_data:
            print("âš ï¸  æ— æ³•è·å–åµŒå…¥å‘é‡ï¼Œè·³è¿‡å‚æ•°ä¼˜åŒ–èšç±»æµ‹è¯•ã€‚")
            return None

        # æ„å»ºé€‚åˆæ™ºèƒ½èšç±»çš„æ•°æ®é¡¹
        items = []
        for i, (text, emb) in enumerate(zip(texts, embeddings_data)):
            items.append({
                "id": i,
                "text": text,
                "embedding": emb
            })
        
        payload = {
            "items": items,
            "config": {
                "umap_n_components": min(8, len(items) - 1),  # è°ƒæ•´ç»´åº¦
                "umap_n_neighbors": min(10, len(items) - 1),  # è°ƒæ•´é‚»å±…æ•°
                "umap_min_dist": 0.0,
                "hdbscan_min_cluster_size": max(2, len(items) // 5),  # æ›´å°çš„æœ€å°ç°‡å¤§å°
                "hdbscan_min_samples": 2,  # å‡å°‘æœ€å°æ ·æœ¬æ•°
                "normalize_embeddings": True
            },
            "optimization": {
                "enabled": True,
                "umap_n_neighbors_range": [min(8, len(items) - 1), min(12, len(items) - 1)],  # è°ƒæ•´æœç´¢èŒƒå›´
                "hdbscan_min_cluster_size_range": [2, max(3, len(items) // 6)],  # æ›´é€‚åˆå°æ•°æ®é›†
                "hdbscan_min_samples_range": [1, 2],  # å‡å°‘æœ€å°æ ·æœ¬æ•°
                "hdbscan_epsilon_range": [0.0, 0.1],  # è°ƒæ•´epsilonèŒƒå›´
                "max_combinations": 8  # å‡å°‘ç»„åˆæ•°
            },
            "return_embeddings": False,
            "return_reduced_embeddings": True
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/clustering/auto",
                json=payload,
                headers=self.headers,
                timeout=180
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                stats = data['clustering_stats']
                optimization = data['optimization_result']
                
                print(f"âœ… å‚æ•°ä¼˜åŒ–èšç±»æˆåŠŸ")
                print(f"   ç°‡æ•°é‡: {stats['n_clusters']}")
                print(f"   å¼‚å¸¸ç‚¹: {stats['n_outliers']} ({stats['outlier_ratio']:.1%})")
                if stats.get('dbcv_score') is not None:
                    print(f"   DBCVåˆ†æ•°: {stats['dbcv_score']:.4f}")
                if optimization['used'] and optimization.get('best_params'):
                    best_params = optimization['best_params']
                    print(f"   æœ€ä½³å‚æ•°ä¼˜åŒ–: ä½¿ç”¨äº†{optimization.get('evaluated_combinations', 'N/A')}ä¸ªç»„åˆ")
                print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                
                return data
            else:
                print(f"âŒ å‚æ•°ä¼˜åŒ–èšç±»å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ å‚æ•°ä¼˜åŒ–èšç±»å¼‚å¸¸: {e}")
            return None

    def test_pipeline(self, texts: List[str]) -> Optional[Dict[str, Any]]:
        """æµ‹è¯•å®Œæ•´æ™ºèƒ½èšç±»æµæ°´çº¿"""
        print(f"\nğŸ”„ æµ‹è¯•å®Œæ•´æ™ºèƒ½èšç±»æµæ°´çº¿ ({len(texts)} ä¸ªæ–‡æœ¬)...")
        
        # æ„å»ºçº¯æ–‡æœ¬é¡¹ç›®ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨ç”ŸæˆåµŒå…¥
        items = []
        for i, text in enumerate(texts):
            items.append({
                "id": i,
                "text": text,
                "metadata": {"source": "test", "index": i}
            })
        
        payload = {
            "items": items,
            "config": {
                "umap_n_components": min(8, len(items) - 1),  # è°ƒæ•´ç»´åº¦
                "umap_n_neighbors": min(10, len(items) - 1),  # è°ƒæ•´é‚»å±…æ•°
                "umap_min_dist": 0.0,
                "hdbscan_min_cluster_size": max(2, len(items) // 5),  # æ›´å°çš„æœ€å°ç°‡å¤§å°
                "hdbscan_min_samples": 2,  # å‡å°‘æœ€å°æ ·æœ¬æ•°
                "normalize_embeddings": True
            },
            "optimization": {
                "enabled": True,
                "umap_n_neighbors_range": [min(8, len(items) - 1), min(12, len(items) - 1)],  # è°ƒæ•´æœç´¢èŒƒå›´
                "hdbscan_min_cluster_size_range": [2, max(3, len(items) // 6)],  # æ›´é€‚åˆå°æ•°æ®é›†
                "hdbscan_min_samples_range": [1, 2],  # å‡å°‘æœ€å°æ ·æœ¬æ•°
                "max_combinations": 6  # å‡å°‘ç»„åˆæ•°
            },
            "content_analysis": {
                "enabled": True,
                "top_n_per_cluster": 3
            },
            "return_embeddings": True,
            "return_reduced_embeddings": True,
            "preserve_original_format": True,
            "include_ai_worker_metadata": True
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/clustering/auto",
                json=payload,
                headers=self.headers,
                timeout=180
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                stats = data['clustering_stats']
                
                print(f"âœ… å®Œæ•´æ™ºèƒ½æµæ°´çº¿æˆåŠŸ")
                if data.get('embeddings'):
                    print(f"   åµŒå…¥ç»´åº¦: {len(data['embeddings'][0])}ç»´")
                else:
                    print(f"   åµŒå…¥ç»´åº¦: ç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ")
                print(f"   ç°‡æ•°é‡: {stats['n_clusters']}")
                if stats.get('dbcv_score') is not None:
                    print(f"   DBCVåˆ†æ•°: {stats['dbcv_score']:.4f}")
                print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                
                return data
            else:
                print(f"âŒ å®Œæ•´æ™ºèƒ½æµæ°´çº¿å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ å®Œæ•´æ™ºèƒ½æµæ°´çº¿å¼‚å¸¸: {e}")
            return None

    def generate_test_embeddings(self, n_samples: int = 15) -> List[List[float]]:
        """ç”Ÿæˆæµ‹è¯•ç”¨çš„åµŒå…¥å‘é‡"""
        print(f"\nğŸ§® ç”Ÿæˆæµ‹è¯•åµŒå…¥å‘é‡ ({n_samples} ä¸ª)...")
        
        # ä½¿ç”¨å‰n_samplesä¸ªæµ‹è¯•æ–‡æœ¬
        test_texts = TEST_TEXTS[:n_samples]
        embeddings = self.test_embeddings(test_texts)
        
        if embeddings:
            print(f"âœ… ç”Ÿæˆäº† {len(embeddings)} ä¸ªåµŒå…¥å‘é‡")
            return embeddings
        else:
            print("âŒ åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
            return []

    def test_clustering_with_embeddings(self) -> Optional[Dict[str, Any]]:
        """æµ‹è¯•ä½¿ç”¨é¢„ç”ŸæˆåµŒå…¥å‘é‡çš„èšç±»"""
        print(f"\nğŸª æµ‹è¯•é¢„ç”ŸæˆåµŒå…¥å‘é‡èšç±»...")
        
        # ç”Ÿæˆæµ‹è¯•åµŒå…¥
        embeddings = self.generate_test_embeddings(12)
        if not embeddings:
            return None
        
        # æ„å»ºå‘é‡é¡¹ç›®
        items = []
        for i, (text, emb) in enumerate(zip(TEST_TEXTS[:len(embeddings)], embeddings)):
            items.append({
                "id": i,
                "text": text,
                "embedding": emb,
                "metadata": {"category": "test", "index": i}
            })
        
        payload = {
            "items": items,
            "config": {
                "umap_n_components": min(6, len(items) - 1),  # è¿›ä¸€æ­¥å‡å°‘ç»´åº¦
                "umap_n_neighbors": min(8, len(items) - 1),   # å‡å°‘é‚»å±…æ•°
                "hdbscan_min_cluster_size": max(2, len(items) // 6),  # æ›´å°çš„æœ€å°ç°‡å¤§å°
                "hdbscan_min_samples": 1,  # æœ€å°æ ·æœ¬æ•°è®¾ä¸º1
                "normalize_embeddings": True
            },
            "content_analysis": {
                "enabled": True,
                "top_n_per_cluster": 2
            },
            "return_embeddings": False,
            "return_reduced_embeddings": True
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/clustering/auto",
                json=payload,
                headers=self.headers,
                timeout=120
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                stats = data['clustering_stats']
                
                print(f"âœ… é¢„ç”ŸæˆåµŒå…¥èšç±»æˆåŠŸ")
                print(f"   ç°‡æ•°é‡: {stats['n_clusters']}")
                print(f"   å¼‚å¸¸ç‚¹: {stats['n_outliers']} ({stats['outlier_ratio']:.1%})")
                if stats.get('dbcv_score') is not None:
                    print(f"   DBCVåˆ†æ•°: {stats['dbcv_score']:.4f}")
                print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
                
                return data
            else:
                print(f"âŒ é¢„ç”ŸæˆåµŒå…¥èšç±»å¤±è´¥: {response.status_code}")
                print(f"   é”™è¯¯: {response.text}")
                return None
        except Exception as e:
            print(f"âŒ é¢„ç”ŸæˆåµŒå…¥èšç±»å¼‚å¸¸: {e}")
            return None


def run_basic_tests(tester: MLServiceTester) -> bool:
    """è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹åŸºç¡€åŠŸèƒ½æµ‹è¯•...")
    
    # 1. å¥åº·æ£€æŸ¥
    if not tester.test_health():
        print("âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®")
        return False
    
    # 2. åµŒå…¥ç”Ÿæˆæµ‹è¯•
    embeddings = tester.test_embeddings(TEST_TEXTS[:5])
    if not embeddings:
        print("âš ï¸  åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
        print("âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®")
        return False
    
    # 3. AI Workerèšç±»æµ‹è¯•
    clustering_result = tester.test_clustering(TEST_TEXTS[:15])
    if not clustering_result:
        print("âš ï¸  èšç±»æµ‹è¯•å¤±è´¥")
        print("âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®")
        return False
    
    print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    return True


def run_advanced_tests(tester: MLServiceTester) -> bool:
    """è¿è¡Œé«˜çº§åŠŸèƒ½æµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹é«˜çº§åŠŸèƒ½æµ‹è¯•...")
    
    success_count = 0
    total_tests = 3
    
    # 1. å‚æ•°ä¼˜åŒ–æµ‹è¯•
    if tester.test_optimized_clustering(TEST_TEXTS[:12]):
        print("âœ… å‚æ•°ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
        success_count += 1
    else:
        print("âŒ å‚æ•°ä¼˜åŒ–æµ‹è¯•å¤±è´¥")
    
    # 2. å®Œæ•´æµæ°´çº¿æµ‹è¯•
    if tester.test_pipeline(TEST_TEXTS[:10]):
        print("âœ… å®Œæ•´æµæ°´çº¿æµ‹è¯•é€šè¿‡")
        success_count += 1
    else:
        print("âŒ å®Œæ•´æµæ°´çº¿æµ‹è¯•å¤±è´¥")
    
    # 3. é¢„ç”ŸæˆåµŒå…¥æµ‹è¯•
    if tester.test_clustering_with_embeddings():
        print("âœ… é¢„ç”ŸæˆåµŒå…¥æµ‹è¯•é€šè¿‡")
        success_count += 1
    else:
        print("âŒ é¢„ç”ŸæˆåµŒå…¥æµ‹è¯•å¤±è´¥")
    
    success_rate = success_count / total_tests
    print(f"\nğŸ“Š é«˜çº§åŠŸèƒ½æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡ ({success_rate:.1%})")
    
    return success_rate >= 0.6  # 60%é€šè¿‡ç‡è®¤ä¸ºæˆåŠŸ


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¤– Meridian ML Service ç»Ÿä¸€æµ‹è¯•å¥—ä»¶")
    print("=" * 70)
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = MLServiceTester()
    
    # è¿è¡ŒåŸºç¡€æµ‹è¯•
    basic_success = run_basic_tests(tester)
    
    if not basic_success:
        print("\nğŸ“š ä½¿ç”¨è¯´æ˜:")
        print("   - ç¡®ä¿æœåŠ¡åœ¨ http://localhost:8081 è¿è¡Œ")
        print("   - API_TOKEN è®¾ç½®ä¸º 'dev-token-123'")
        print("   - å·²å®‰è£…èšç±»ä¾èµ–: umap-learn hdbscan scikit-learn")
        return
    
    # è¿è¡Œé«˜çº§æµ‹è¯•
    advanced_success = run_advanced_tests(tester)
    
    # æœ€ç»ˆç»“æœ
    if basic_success and advanced_success:
        print("\nğŸ‰ ç»Ÿä¸€æµ‹è¯•å¥—ä»¶é€šè¿‡ï¼")
    elif basic_success:
        print("\nâš ï¸  åŸºç¡€åŠŸèƒ½æ­£å¸¸ï¼Œä½†éƒ¨åˆ†é«˜çº§åŠŸèƒ½æœ‰é—®é¢˜")
        print("å»ºè®®æ£€æŸ¥ä¼˜åŒ–å’Œå†…å®¹åˆ†æç›¸å…³é…ç½®")
    else:
        print("\nâŒ æµ‹è¯•å¥—ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®")


if __name__ == "__main__":
    main() 