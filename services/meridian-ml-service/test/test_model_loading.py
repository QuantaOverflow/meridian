#!/usr/bin/env python3
"""
æ¨¡å‹åŠ è½½è¯Šæ–­è„šæœ¬
æµ‹è¯•ä¸åŒçš„æ¨¡å‹åŠ è½½æ–¹å¼ä»¥è¯Šæ–­é—®é¢˜
"""

import os
import sys
from pathlib import Path

def print_environment():
    """æ‰“å°ç¯å¢ƒå˜é‡å’Œè·¯å¾„ä¿¡æ¯"""
    print("=== ç¯å¢ƒå˜é‡ä¿¡æ¯ ===")
    print(f"HF_HOME: {os.getenv('HF_HOME', 'Not set')}")
    print(f"HF_HUB_CACHE: {os.getenv('HF_HUB_CACHE', 'Not set')}")
    print(f"TRANSFORMERS_CACHE: {os.getenv('TRANSFORMERS_CACHE', 'Not set')}")
    print(f"HF_HUB_OFFLINE: {os.getenv('HF_HUB_OFFLINE', 'Not set')}")
    print(f"TRANSFORMERS_OFFLINE: {os.getenv('TRANSFORMERS_OFFLINE', 'Not set')}")
    print(f"EMBEDDING_MODEL_NAME: {os.getenv('EMBEDDING_MODEL_NAME', 'Not set')}")
    print()

def check_cache_directories():
    """æ£€æŸ¥ç¼“å­˜ç›®å½•ç»“æ„"""
    print("=== ç¼“å­˜ç›®å½•æ£€æŸ¥ ===")
    
    hf_home = os.getenv('HF_HOME', '/home/appuser/.cache/huggingface')
    cache_path = Path(hf_home)
    
    print(f"æ£€æŸ¥è·¯å¾„: {cache_path}")
    print(f"è·¯å¾„å­˜åœ¨: {cache_path.exists()}")
    
    if cache_path.exists():
        print("å­ç›®å½•:")
        for item in cache_path.iterdir():
            if item.is_dir():
                print(f"  ğŸ“ {item.name}")
                # æ£€æŸ¥æ¨¡å‹ç›®å½•
                if "multilingual-e5-small" in item.name:
                    print(f"     -> æ‰¾åˆ°ç›®æ ‡æ¨¡å‹ç›®å½•")
                    # æ£€æŸ¥blobæ–‡ä»¶
                    blobs_dir = item / "blobs"
                    if blobs_dir.exists():
                        blob_count = len(list(blobs_dir.glob("*")))
                        print(f"     -> Blobs: {blob_count} ä¸ªæ–‡ä»¶")
            else:
                print(f"  ğŸ“„ {item.name}")
    
    # æ£€æŸ¥hubç›®å½•
    hub_path = cache_path / "hub"
    print(f"\nHubç›®å½• ({hub_path}):")
    print(f"å­˜åœ¨: {hub_path.exists()}")
    
    if hub_path.exists():
        for item in hub_path.iterdir():
            if item.is_dir():
                print(f"  ğŸ“ {item.name}")
                if "multilingual-e5-small" in item.name:
                    print(f"     -> æ‰¾åˆ°ç›®æ ‡æ¨¡å‹ç›®å½•")
                    # æ£€æŸ¥è¯¦ç»†ç»“æ„
                    refs_dir = item / "refs"
                    blobs_dir = item / "blobs"
                    snapshots_dir = item / "snapshots"
                    
                    print(f"     -> refs: {refs_dir.exists()}")
                    print(f"     -> blobs: {blobs_dir.exists()}")
                    print(f"     -> snapshots: {snapshots_dir.exists()}")
                    
                    if blobs_dir.exists():
                        blob_count = len(list(blobs_dir.glob("*")))
                        print(f"     -> Blobs: {blob_count} ä¸ªæ–‡ä»¶")
    print()

def test_model_loading():
    """æµ‹è¯•ä¸åŒçš„æ¨¡å‹åŠ è½½æ–¹å¼"""
    print("=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    model_name = os.getenv("EMBEDDING_MODEL_NAME", "intfloat/multilingual-e5-small")
    print(f"æ¨¡å‹åç§°: {model_name}")
    
    try:
        from transformers import AutoModel, AutoTokenizer
        print("âœ… Transformersåº“å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Transformersåº“å¯¼å…¥å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•1: é»˜è®¤åŠ è½½
    print("\n--- æµ‹è¯•1: é»˜è®¤åŠ è½½ ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        print("âœ… é»˜è®¤åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é»˜è®¤åŠ è½½å¤±è´¥: {e}")
    
    # æµ‹è¯•2: ç¦»çº¿æ¨¡å¼
    print("\n--- æµ‹è¯•2: ç¦»çº¿æ¨¡å¼ ---")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            local_files_only=True
        )
        model = AutoModel.from_pretrained(
            model_name, 
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ… ç¦»çº¿æ¨¡å¼åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
    
    # æµ‹è¯•3: æŒ‡å®šç¼“å­˜ç›®å½•
    print("\n--- æµ‹è¯•3: æŒ‡å®šç¼“å­˜ç›®å½• ---")
    try:
        cache_dir = os.getenv('HF_HOME', '/home/appuser/.cache/huggingface')
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            cache_dir=cache_dir
        )
        model = AutoModel.from_pretrained(
            model_name, 
            trust_remote_code=True,
            cache_dir=cache_dir
        )
        print("âœ… æŒ‡å®šç¼“å­˜ç›®å½•åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æŒ‡å®šç¼“å­˜ç›®å½•åŠ è½½å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Meridian ML Service - æ¨¡å‹åŠ è½½è¯Šæ–­")
    print("=" * 50)
    
    print_environment()
    check_cache_directories()
    test_model_loading()
    
    print("\nğŸ è¯Šæ–­å®Œæˆ")

if __name__ == "__main__":
    main() 