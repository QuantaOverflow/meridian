#!/usr/bin/env python3
"""
å°æ•°æ®é›†èšç±»ä¼˜åŒ–æµ‹è¯•
é’ˆå¯¹12ç¯‡æ–‡ç« è¿™ç§å°è§„æ¨¡æ•°æ®é›†ï¼Œæµ‹è¯•ä¸åŒçš„å‚æ•°é…ç½®
"""

import asyncio
import httpx
import json
from typing import List, Dict, Any
import os

# MLæœåŠ¡é…ç½®
ML_SERVICE_BASE_URL = 'http://localhost:8081'
API_TOKEN = 'dev-token-123'

async def load_mock_articles(filename: str = "mock_articles.json") -> List[Dict[str, Any]]:
    """åŠ è½½ç”Ÿæˆçš„æ¨¡æ‹Ÿæ–‡ç« """
    
    if not os.path.exists(filename):
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_mock_articles.py ç”Ÿæˆæ¨¡æ‹Ÿæ–‡ç« ")
        return []
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    articles = data.get('articles', [])
    print(f"ğŸ“š åŠ è½½äº† {len(articles)} ç¯‡æ¨¡æ‹Ÿæ–‡ç« ")
    
    return articles

async def test_optimized_small_dataset_clustering(articles: List[Dict[str, Any]]):
    """æµ‹è¯•é’ˆå¯¹å°æ•°æ®é›†ä¼˜åŒ–çš„èšç±»å‚æ•°"""
    
    headers = {'X-API-Token': API_TOKEN}
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = []
    article_info = []
    
    for article in articles:
        # ä½¿ç”¨æ ‡é¢˜+å†…å®¹çš„å‰300å­—ç¬¦
        content = article.get('content', '')
        text = f"{article['title']}\n\n{content[:300]}..."
        test_texts.append(text)
        
        article_info.append({
            'id': article['id'],
            'title': article['title'],
            'category': article['category'],
            'content_length': len(content)
        })
    
    print(f"\nğŸ§ª å¼€å§‹å°æ•°æ®é›†ä¼˜åŒ–èšç±»æµ‹è¯• ({len(test_texts)} ç¯‡æ–‡ç« )")
    
    async with httpx.AsyncClient(timeout=120) as client:
        
        # æµ‹è¯•é…ç½®1: å®½æ¾çš„HDBSCANå‚æ•°
        print("\n1ï¸âƒ£ æµ‹è¯•é…ç½®1: å®½æ¾HDBSCANå‚æ•°...")
        
        config1 = {
            'texts': test_texts,
            'clustering_config': {
                'umap_n_components': 5,  # é™ä½ç»´åº¦
                'umap_n_neighbors': 8,   # å‡å°‘é‚»å±…æ•°
                'umap_min_dist': 0.1,    # å¢åŠ æœ€å°è·ç¦»
                'umap_metric': 'cosine',
                'hdbscan_min_cluster_size': 2,  # æœ€å°ç°‡å¤§å°æ”¹ä¸º2
                'hdbscan_min_samples': 1,       # æœ€å°æ ·æœ¬æ•°æ”¹ä¸º1
                'hdbscan_cluster_selection_epsilon': 0.1,  # å…è®¸ä¸€å®šå¯†åº¦æ³¢åŠ¨
                'hdbscan_metric': 'euclidean',
                'normalize_embeddings': True
            }
        }
        
        try:
            response = await client.post(
                f'{ML_SERVICE_BASE_URL}/embeddings-and-clustering',
                json=config1,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                await analyze_clustering_result(result['clustering_result'], article_info, "å®½æ¾HDBSCAN")
            else:
                print(f"âŒ é…ç½®1å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ é…ç½®1å¼‚å¸¸: {e}")
        
        # æµ‹è¯•é…ç½®2: æç®€å‚æ•°
        print("\n2ï¸âƒ£ æµ‹è¯•é…ç½®2: æç®€å‚æ•°...")
        
        config2 = {
            'texts': test_texts,
            'clustering_config': {
                'umap_n_components': 3,  # è¿›ä¸€æ­¥é™ä½ç»´åº¦
                'umap_n_neighbors': 5,   # æ›´å°‘é‚»å±…
                'umap_min_dist': 0.2,    # æ›´å¤§æœ€å°è·ç¦»
                'umap_metric': 'cosine',
                'hdbscan_min_cluster_size': 2,  # æœ€å°ç°‡å¤§å°
                'hdbscan_min_samples': 1,       # æœ€å°æ ·æœ¬æ•°
                'hdbscan_cluster_selection_epsilon': 0.2,  # æ›´å®½æ¾çš„å¯†åº¦è¦æ±‚
                'hdbscan_metric': 'euclidean',
                'normalize_embeddings': True
            }
        }
        
        try:
            response = await client.post(
                f'{ML_SERVICE_BASE_URL}/embeddings-and-clustering',
                json=config2,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                await analyze_clustering_result(result['clustering_result'], article_info, "æç®€å‚æ•°")
            else:
                print(f"âŒ é…ç½®2å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ é…ç½®2å¼‚å¸¸: {e}")
        
        # æµ‹è¯•é…ç½®3: ä¼˜åŒ–çš„ç½‘æ ¼æœç´¢
        print("\n3ï¸âƒ£ æµ‹è¯•é…ç½®3: å°æ•°æ®é›†ä¸“ç”¨ç½‘æ ¼æœç´¢...")
        
        config3 = {
            'texts': test_texts,
            'use_optimization': True,
            'grid_config': {
                'umap_n_neighbors': [3, 5, 8],         # å°æ•°æ®é›†é€‚ç”¨çš„é‚»å±…æ•°
                'umap_n_components': 3,                 # å›ºå®šä½ç»´åº¦
                'umap_min_dist': 0.1,                   # å›ºå®šä¸­ç­‰è·ç¦»
                'umap_metric': 'cosine',                # å›ºå®šcosine
                'hdbscan_min_cluster_size': [2, 3],     # å°ç°‡å¤§å°
                'hdbscan_min_samples': [1, 2],          # å°æ ·æœ¬æ•°
                'hdbscan_epsilon': [0.1, 0.2, 0.3],     # å®½æ¾å¯†åº¦
                'hdbscan_metric': 'euclidean'           # å›ºå®šeuclidean
            }
        }
        
        try:
            response = await client.post(
                f'{ML_SERVICE_BASE_URL}/clustering/optimized',
                json=config3,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                await analyze_clustering_result(result, article_info, "å°æ•°æ®é›†ç½‘æ ¼æœç´¢")
            else:
                print(f"âŒ é…ç½®3å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ é…ç½®3å¼‚å¸¸: {e}")

async def analyze_clustering_result(result: Dict[str, Any], article_info: List[Dict[str, Any]], method_name: str):
    """åˆ†æèšç±»ç»“æœ"""
    
    print(f"\nğŸ“Š {method_name}ç»“æœåˆ†æ:")
    
    # åŸºç¡€ç»Ÿè®¡
    stats = result.get('clustering_stats', {})
    cluster_labels = result.get('cluster_labels', [])
    
    # æ„å»ºclusterç»“æ„ç”¨äºåˆ†æ
    original_clusters_list = {}
    for i, label in enumerate(cluster_labels):
        if label not in original_clusters_list:
            original_clusters_list[label] = []
        original_clusters_list[label].append(i)

    clusters = sorted(
        [{'cluster_id': label, 'members': indices} for label, indices in original_clusters_list.items()],
        key=lambda x: x['cluster_id'] if x['cluster_id'] != -1 else float('inf')
    )

    print(f"   ğŸ”¢ åŸºç¡€ç»Ÿè®¡:")
    print(f"      - æ€»æ–‡ç« æ•°: {stats.get('n_samples', len(article_info))}")
    print(f"      - èšç±»æ•°é‡: {stats.get('n_clusters', len([k for k in original_clusters_list if k != -1]))}")
    print(f"      - å™ªå£°ç‚¹æ•°: {stats.get('n_outliers', len(original_clusters_list.get(-1, [])))}")
    clustered_count = len(cluster_labels) - len(original_clusters_list.get(-1, []))
    clustering_rate = clustered_count / len(cluster_labels) if cluster_labels else 0
    print(f"      - èšç±»ç‡: {clustering_rate:.1%}")
    
    if result.get('optimization', {}).get('used'):
        dbcv_score = result.get('optimization', {}).get('best_dbcv_score')
        if dbcv_score is not None:
            print(f"      - ä¼˜åŒ–åˆ†æ•° (DBCV): {dbcv_score:.3f}")
    
    # èšç±»è¯¦æƒ…
    if clusters:
        print(f"\n   ğŸ“ èšç±»è¯¦æƒ…:")
        
        for cluster in clusters:
            cluster_id = cluster['cluster_id']
            member_indices = cluster['members']
            
            if cluster_id == -1:
                print(f"      ğŸ”¸ å™ªå£°ç‚¹ ({len(member_indices)} ç¯‡):")
            else:
                print(f"      ğŸ“ èšç±» {cluster_id} ({len(member_indices)} ç¯‡):")
            
            # æ˜¾ç¤ºè¯¥èšç±»ä¸­çš„æ–‡ç« 
            cluster_articles = [article_info[i] for i in member_indices if i < len(article_info)]
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_count = {}
            for article in cluster_articles:
                cat = article['category']
                category_count[cat] = category_count.get(cat, 0) + 1
            
            print(f"         ç±»åˆ«åˆ†å¸ƒ: {dict(category_count)}")
            
            # æ˜¾ç¤ºæ–‡ç« æ ‡é¢˜
            for i, article in enumerate(cluster_articles[:2]):  # åªæ˜¾ç¤ºå‰2ç¯‡
                print(f"         â€¢ {article['title'][:40]}...")
            
            if len(cluster_articles) > 2:
                print(f"         ... è¿˜æœ‰ {len(cluster_articles) - 2} ç¯‡")
    
    # èšç±»è´¨é‡è¯„ä¼°
    await evaluate_clustering_quality(clusters, article_info)

async def evaluate_clustering_quality(clusters: List[Dict[str, Any]], article_info: List[Dict[str, Any]]):
    """è¯„ä¼°èšç±»è´¨é‡"""
    
    print(f"\n   ğŸ¯ èšç±»è´¨é‡è¯„ä¼°:")
    
    # è®¡ç®—ç±»åˆ«çº¯åº¦
    total_purity = 0
    valid_clusters = 0
    
    for cluster in clusters:
        if cluster['cluster_id'] == -1:  # è·³è¿‡å™ªå£°ç‚¹
            continue
            
        member_indices = cluster['members']
        if len(member_indices) < 1:  # è°ƒæ•´ä¸ºè‡³å°‘1ç¯‡æ–‡ç« 
            continue
            
        # è·å–è¯¥èšç±»çš„æ‰€æœ‰æ–‡ç« ç±»åˆ«
        categories = []
        for idx in member_indices:
            if idx < len(article_info):
                categories.append(article_info[idx]['category'])
        
        if categories:
            # è®¡ç®—ä¸»è¦ç±»åˆ«çš„çº¯åº¦
            category_count = {}
            for cat in categories:
                category_count[cat] = category_count.get(cat, 0) + 1
            
            max_count = max(category_count.values())
            purity = max_count / len(categories)
            total_purity += purity
            valid_clusters += 1
            
            main_category = max(category_count, key=category_count.get)
            print(f"      èšç±» {cluster['cluster_id']}: çº¯åº¦ {purity:.1%} (ä¸»è¦ç±»åˆ«: {main_category})")
    
    if valid_clusters > 0:
        avg_purity = total_purity / valid_clusters
        print(f"      ğŸ“ˆ å¹³å‡çº¯åº¦: {avg_purity:.1%}")
        
        if avg_purity >= 0.8:
            print(f"      âœ… èšç±»è´¨é‡: ä¼˜ç§€")
        elif avg_purity >= 0.6:
            print(f"      ğŸ”¶ èšç±»è´¨é‡: è‰¯å¥½")
        else:
            print(f"      âš ï¸ èšç±»è´¨é‡: éœ€è¦æ”¹è¿›")
    else:
        print(f"      âŒ æ²¡æœ‰æœ‰æ•ˆèšç±»ç”Ÿæˆ")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å°æ•°æ®é›†èšç±»ä¼˜åŒ–æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½æ¨¡æ‹Ÿæ–‡ç« 
    articles = await load_mock_articles('/Users/shiwenjie/Developer/meridian/services/meridian-ml-service/test/mock_articles.json')
    
    if not articles:
        return
    
    # æ‰§è¡Œä¼˜åŒ–èšç±»æµ‹è¯•
    await test_optimized_small_dataset_clustering(articles)
    
    print(f"\nğŸ‰ å°æ•°æ®é›†èšç±»ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ å»ºè®®:")
    print("   1. å¯¹äºå°æ•°æ®é›†ï¼Œä½¿ç”¨min_cluster_size=2")
    print("   2. é€‚å½“å¢åŠ epsilonå‚æ•°å…è®¸å¯†åº¦æ³¢åŠ¨")
    print("   3. é™ä½UMAPçš„ç»´åº¦å’Œé‚»å±…æ•°")
    print("   4. è€ƒè™‘ä½¿ç”¨ä¸åŒçš„åµŒå…¥æ¨¡å‹æˆ–é¢„å¤„ç†ç­–ç•¥")

if __name__ == "__main__":
    asyncio.run(main()) 