#!/usr/bin/env python3
"""
ä½¿ç”¨æ¨¡æ‹Ÿæ–‡ç« æµ‹è¯•èšç±»åŠŸèƒ½
è¯»å–ç”Ÿæˆçš„æ¨¡æ‹Ÿæ–‡ç« ï¼Œæµ‹è¯•MLæœåŠ¡çš„èšç±»æ•ˆæœ
"""

import asyncio
import httpx
import json
from typing import List, Dict, Any
import os

# MLæœåŠ¡é…ç½®
ML_SERVICE_BASE_URL = 'http://localhost:8081'
API_TOKEN = 'dev-token-123'

async def load_mock_articles(filename: str = "mock_articles.json") -> List[Dict[str, Any]]:
    """åŠ è½½ç”Ÿæˆçš„æ¨¡æ‹Ÿæ–‡ç« """
    
    if not os.path.exists(filename):
        print(f"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_mock_articles.py ç”Ÿæˆæ¨¡æ‹Ÿæ–‡ç« ")
        return []
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    articles = data.get('articles', [])
    print(f"ğŸ“š åŠ è½½äº† {len(articles)} ç¯‡æ¨¡æ‹Ÿæ–‡ç« ")
    
    # æ˜¾ç¤ºæ–‡ç« ç»Ÿè®¡
    categories = {}
    total_length = 0
    
    for article in articles:
        cat = article.get('category', 'unknown')
        categories[cat] = categories.get(cat, 0) + 1
        content_length = len(article.get('content', ''))
        total_length += content_length
    
    print(f"ğŸ“Š æ–‡ç« ç»Ÿè®¡:")
    for cat, count in categories.items():
        print(f"   - {cat}: {count} ç¯‡")
    print(f"   - å¹³å‡é•¿åº¦: {total_length // len(articles):,} å­—ç¬¦")
    
    return articles

async def test_clustering(articles: List[Dict[str, Any]]):
    """æµ‹è¯•èšç±»åŠŸèƒ½"""
    
    headers = {'X-API-Token': API_TOKEN}
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = []
    article_info = []
    
    for article in articles:
        # ä½¿ç”¨æ ‡é¢˜+å†…å®¹çš„å‰500å­—ç¬¦ä½œä¸ºæµ‹è¯•æ–‡æœ¬
        content = article.get('content', '')
        text = f"{article['title']}\n\n{content[:500]}..."
        test_texts.append(text)
        
        article_info.append({
            'id': article['id'],
            'title': article['title'],
            'category': article['category'],
            'content_length': len(content)
        })
    
    print(f"\nğŸ§ª å¼€å§‹èšç±»æµ‹è¯• ({len(test_texts)} ç¯‡æ–‡ç« )")
    
    async with httpx.AsyncClient(timeout=120) as client:
        # 1. æµ‹è¯•æ ‡å‡†èšç±»
        print("\n1ï¸âƒ£ æµ‹è¯•æ ‡å‡†èšç±»...")
        
        payload = {'texts': test_texts}
        
        try:
            response = await client.post(
                f'{ML_SERVICE_BASE_URL}/clustering',
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                await analyze_clustering_result(result, article_info, "æ ‡å‡†èšç±»")
            else:
                print(f"âŒ æ ‡å‡†èšç±»å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ æ ‡å‡†èšç±»å¼‚å¸¸: {e}")
        
        # 2. æµ‹è¯•ä¼˜åŒ–èšç±»
        print("\n2ï¸âƒ£ æµ‹è¯•ä¼˜åŒ–èšç±»...")
        
        payload = {
            'texts': test_texts,
            'use_optimization': True
        }
        
        try:
            response = await client.post(
                f'{ML_SERVICE_BASE_URL}/clustering/optimized',
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                result = response.json()
                await analyze_clustering_result(result, article_info, "ä¼˜åŒ–èšç±»")
            else:
                print(f"âŒ ä¼˜åŒ–èšç±»å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–èšç±»å¼‚å¸¸: {e}")

async def analyze_clustering_result(result: Dict[str, Any], article_info: List[Dict[str, Any]], method_name: str):
    """åˆ†æèšç±»ç»“æœ"""
    
    print(f"\nğŸ“Š {method_name}ç»“æœåˆ†æ:")
    
    # åŸºç¡€ç»Ÿè®¡
    stats = result.get('clustering_stats', {})
    
    # ä»mlæœåŠ¡å“åº”ä¸­è·å–cluster_labelså’Œcluster_content
    cluster_labels = result.get('cluster_labels', [])
    cluster_content_map = result.get('cluster_content', {}) # è¿™æ˜¯ä¸ªå­—å…¸ï¼Œkeyæ˜¯cluster_idï¼Œvalueæ˜¯æ–‡ç« å†…å®¹åˆ—è¡¨
    
    # æ ¹æ®cluster_labelså’Œarticle_infoæ„å»ºç¬¦åˆåŸå…ˆæµ‹è¯•é€»è¾‘çš„clustersåˆ—è¡¨
    # original_clusters_list ç”¨äºå­˜å‚¨æ¯ä¸ªcluster_idå¯¹åº”çš„æ–‡ç« ç´¢å¼•
    original_clusters_list = {}
    for i, label in enumerate(cluster_labels):
        if label not in original_clusters_list:
            original_clusters_list[label] = []
        original_clusters_list[label].append(i)

    # è½¬æ¢æˆæµ‹è¯•è„šæœ¬æœŸæœ›çš„æ ¼å¼
    # clusters = [{'cluster_id': label, 'members': indices} for label, indices in original_clusters_list.items()]
    # ç¡®ä¿å™ªå£°ç‚¹ï¼ˆ-1ï¼‰æ’åœ¨æœ€å
    clusters = sorted(
        [{'cluster_id': label, 'members': indices} for label, indices in original_clusters_list.items()],
        key=lambda x: x['cluster_id'] if x['cluster_id'] != -1 else float('inf')
    )


    print(f"   ğŸ”¢ åŸºç¡€ç»Ÿè®¡:")
    print(f"      - æ€»æ–‡ç« æ•°: {stats.get('total_items', len(article_info))}") # ä½¿ç”¨article_infoçš„é•¿åº¦ä½œä¸ºæ€»æ–‡ç« æ•°
    print(f"      - èšç±»æ•°é‡: {stats.get('n_clusters', len([k for k in original_clusters_list if k != -1]))}") # ç»Ÿè®¡éå™ªå£°ç‚¹çš„èšç±»æ•°é‡
    print(f"      - å™ªå£°ç‚¹æ•°: {stats.get('n_noise', len(original_clusters_list.get(-1, [])))}") # ç»Ÿè®¡å™ªå£°ç‚¹æ•°é‡
    print(f"      - èšç±»ç‡: {stats.get('clustered_ratio', 0):.1%}") # èšç±»ç‡å¯èƒ½éœ€è¦é‡æ–°è®¡ç®—æˆ–ä»statsä¸­è·å–
    
    if 'optimization_score' in result:
        print(f"      - ä¼˜åŒ–åˆ†æ•°: {result['optimization_score']:.3f}")
    elif result.get('optimization', {}).get('used') and result.get('optimization', {}).get('best_dbcv_score') is not None:
        print(f"      - ä¼˜åŒ–åˆ†æ•° (DBCV): {result['optimization']['best_dbcv_score']:.3f}") # ä»ä¼˜åŒ–ç»“æœä¸­è·å–DBCVåˆ†æ•°
    
    # èšç±»è¯¦æƒ…
    if clusters: # ç°åœ¨clustersä¼šæ˜¯æ­£ç¡®çš„æ ¼å¼
        print(f"\n   ğŸ“ èšç±»è¯¦æƒ…:")
        
        for cluster in clusters:
            cluster_id = cluster['cluster_id']
            member_indices = cluster['members']
            
            if cluster_id == -1:
                print(f"      ğŸ”¸ å™ªå£°ç‚¹ ({len(member_indices)} ç¯‡):")
            else:
                print(f"      ğŸ“ èšç±» {cluster_id} ({len(member_indices)} ç¯‡):")
            
            # æ˜¾ç¤ºè¯¥èšç±»ä¸­çš„æ–‡ç« 
            cluster_articles = [article_info[i] for i in member_indices if i < len(article_info)]
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_count = {}
            for article in cluster_articles:
                cat = article['category']
                category_count[cat] = category_count.get(cat, 0) + 1
            
            print(f"         ç±»åˆ«åˆ†å¸ƒ: {dict(category_count)}")
            
            # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« æ ‡é¢˜
            for i, article in enumerate(cluster_articles[:3]):
                print(f"         â€¢ {article['title'][:50]}...")
            
            if len(cluster_articles) > 3:
                print(f"         ... è¿˜æœ‰ {len(cluster_articles) - 3} ç¯‡")
    
    # èšç±»è´¨é‡è¯„ä¼°
    await evaluate_clustering_quality(clusters, article_info) # è¿™é‡Œä½¿ç”¨æ–°çš„clusters

async def evaluate_clustering_quality(clusters: List[Dict[str, Any]], article_info: List[Dict[str, Any]]):
    """è¯„ä¼°èšç±»è´¨é‡"""
    
    print(f"\n   ğŸ¯ èšç±»è´¨é‡è¯„ä¼°:")
    
    # è®¡ç®—ç±»åˆ«çº¯åº¦ (åŒä¸€èšç±»ä¸­ç›¸åŒç±»åˆ«æ–‡ç« çš„æ¯”ä¾‹)
    total_purity = 0
    valid_clusters = 0
    
    for cluster in clusters:
        if cluster['cluster_id'] == -1:  # è·³è¿‡å™ªå£°ç‚¹
            continue
            
        member_indices = cluster['members']
        if len(member_indices) < 2:  # è·³è¿‡å•æ–‡ç« èšç±»
            continue
            
        # è·å–è¯¥èšç±»çš„æ‰€æœ‰æ–‡ç« ç±»åˆ«
        categories = []
        for idx in member_indices:
            if idx < len(article_info):
                categories.append(article_info[idx]['category'])
        
        if categories:
            # è®¡ç®—ä¸»è¦ç±»åˆ«çš„çº¯åº¦
            category_count = {}
            for cat in categories:
                category_count[cat] = category_count.get(cat, 0) + 1
            
            max_count = max(category_count.values())
            purity = max_count / len(categories)
            total_purity += purity
            valid_clusters += 1
            
            main_category = max(category_count, key=category_count.get)
            print(f"      èšç±» {cluster['cluster_id']}: çº¯åº¦ {purity:.1%} (ä¸»è¦ç±»åˆ«: {main_category})")
    
    if valid_clusters > 0:
        avg_purity = total_purity / valid_clusters
        print(f"      ğŸ“ˆ å¹³å‡çº¯åº¦: {avg_purity:.1%}")
        
        if avg_purity >= 0.8:
            print(f"      âœ… èšç±»è´¨é‡: ä¼˜ç§€")
        elif avg_purity >= 0.6:
            print(f"      ğŸ”¶ èšç±»è´¨é‡: è‰¯å¥½")
        else:
            print(f"      âš ï¸ èšç±»è´¨é‡: éœ€è¦æ”¹è¿›")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ æ¨¡æ‹Ÿæ–‡ç« èšç±»æµ‹è¯•")
    print("=" * 50)
    
    # åŠ è½½æ¨¡æ‹Ÿæ–‡ç« 
    articles = await load_mock_articles('/Users/shiwenjie/Developer/meridian/services/meridian-ml-service/test/mock_articles.json')
    
    if not articles:
        return
    
    # æ‰§è¡Œèšç±»æµ‹è¯•
    await test_clustering(articles)
    
    print(f"\nğŸ‰ èšç±»æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ å¯ä»¥æ ¹æ®ç»“æœè°ƒæ•´èšç±»å‚æ•°ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ")

if __name__ == "__main__":
    asyncio.run(main()) 